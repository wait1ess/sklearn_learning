{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13 种机器学习算法概述（附 Python代码）\n",
    "机器学习算法分为三种类型。\n",
    "1)监督式学习\n",
    "工作原理：该算法由一个目标/结果变量（或因变量）组成，该变量（或因变量）由给定的一组预测器（自变量）进行预测得到。使用这些变量集，我们可以生成输入映射到期望输出的函数。通过训练算法模型，让模型在训练数据上得到期望的准确度。监督学习的例子包括：回归、决策树、随机森林、KNN、Logistic 回归等。\n",
    "\n",
    "2)非监督式学习\n",
    "工作原理：该算法没有任何目标/结果变量（或因变量）来预测或估计。它用于对样本中的不同类别进行聚类，广泛用于在不知道标签的情况下对不同群体进行划分。无监督学习的例子包括：Apriori 算法，k-均值。\n",
    "\n",
    "3)增强学习\n",
    "工作原理：机器被训练来做出特定的决定。它是这样工作的：机器通过反复试验不断训练自己，从过去的经验中学习，并试图运用最好的知识，作出准确的决策。强化学习的例子包括：马尔可夫（Markov）决策过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常用的机器学习算法列表\n",
    "- 线性回归\n",
    "- 逻辑回归\n",
    "- 决策树\n",
    "- SVM\n",
    "- 朴素贝叶斯\n",
    "- k 近邻\n",
    "- k-聚类\n",
    "- 随机森林\n",
    "- 降维算法\n",
    "- 梯度提升算法  GBM,  XGBoost,  LightGBM , CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 线性回归\n",
    "拟合一条最佳直线来建立自变量（x）和因变量（y）之间的关系\n",
    "线性回归主要有两类：简单线性回归和多元线性回归。简单线性回归的特点是只有一个自变量。多元线性回归的特征是有多个（大于 1）独立变量。当然，为了找到最佳拟合线，可以使用多项式拟合或曲线拟合，分别称为多项式回归和曲线回归。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Library\n",
    "#Import other necessary libraries like pandas, numpy...\n",
    "from sklearn import linear_model\n",
    "#Load Train and Test datasets\n",
    "#Identify feature and response variable(s) and values must be numeric and numpy arrays\n",
    "x_train=input_variables_values_training_datasets\n",
    "y_train=target_variables_values_training_datasets\n",
    "x_test=input_variables_values_test_datasets\n",
    "\n",
    "linear = linear_model.LinearRegression()                                   # Create linear regression object\n",
    "\n",
    "linear.fit(x_train, y_train)                                               # Train the model using the training sets and check score\n",
    "linear.score(x_train, y_train)\n",
    "\n",
    "print('Coefficient: \\n', linear.coef_)                                     # Equation coefficient and Intercept\n",
    "print('Intercept: \\n', linear.intercept_)\n",
    "\n",
    "predicted= linear.predict(x_test)                                          # Predict Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 逻辑回归\n",
    "逻辑回归（Logistic Regression）是一种分类算法\n",
    "根据给定的一组自变量估计离散值（二进制值如 0/1、是/否、真/假）。简单地说，它通过将数据拟合到 logit 函数来预测事件发生的概率。因此称之为 logit 回归。因为它预测的是概率值，所以输出介于 0 到 1 之间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果的对数几率就是预测变量的线性组合。\n",
    "odds= p/ (1-p) = probability of event occurrence / probability of not event occurrence\n",
    "ln(odds) = ln(p/(1-p))\n",
    "logit(p) = ln(p/(1-p)) = b0+b1X1+b2X2+b3X3....+bkXk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Library\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\n",
    "\n",
    "model = LogisticRegression()                 # Create logistic regression object\n",
    "\n",
    "model.fit(X, y)                              # Train the model using the training sets and check score\n",
    "model.score(X, y)\n",
    "\n",
    "print('Coefficient: \\n', model.coef_)        #Equation coefficient and Intercept\n",
    "print('Intercept: \\n', model.intercept_)\n",
    "\n",
    "predicted= model.predict(x_test)             #Predict Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有许多方法可以提高模型性能：\n",
    "交互作用项\n",
    "移除特征\n",
    "正则化技术（https://www.analyticsvidhya.com/blog/2015/02/avoid-over-fitting-regularization/）\n",
    "使用更复杂的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 决策树\n",
    "决策树（Decision Tree）是主要用于分类问题的监督式学习算法。因变量可以是离散的也可以是连续的。在该算法中，我们将数据划分成两个或更多的组。划分的准则是基于最重要的属性/自变量，尽可能让不同组别之间的差别大一些。\n",
    "决策树简化教程：\n",
    "https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/\n",
    "实现不同分组的方法有很多，例如 Gini、信息增益、卡方、熵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Library\n",
    "#Import other necessary libraries like pandas, numpy...\n",
    "from sklearn import tree\n",
    "#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\n",
    "                                                          # Create tree object \n",
    "model = tree.DecisionTreeClassifier(criterion='gini')     # for classification, here you can change the algorithm as gini or entropy (information gain) by default it is gini  \n",
    "                                                          # model = tree.DecisionTreeRegressor() for regression\n",
    "\n",
    "model.fit(X, y)                                           # Train the model using the training sets and check score\n",
    "model.score(X, y)\n",
    "\n",
    "predicted= model.predict(x_test)                          #Predict Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SVM（支持向量机）\n",
    "SVM 是一种分类算法。在这个算法中，我们将每个数据项绘制为 n 维空间中的一个点（其中 n 是特征个数），每个特征都对应特定的坐标。\n",
    "寻找一条直线来划分这两类数据。可选择的直线很多，但是最好的一条应该是能够使两类中最靠近直线的点距离最远。\n",
    "然后，测试数据落在该直线的哪一边，就可以判定数据属于哪一类。\n",
    "https://www.analyticsvidhya.com/blog/2014/10/support-vector-machine-simplified/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Library\n",
    "from sklearn import svm\n",
    "#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\n",
    "# Create SVM classification object \n",
    "model = svm.svc()                      # there is various option associated with it, this is simple for classification. You can refer link, for mo# re detail.\n",
    "\n",
    "model.fit(X, y)                        # Train the model using the training sets and check score\n",
    "model.score(X, y)\n",
    "\n",
    "predicted= model.predict(x_test)       #Predict Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 朴素贝叶斯\n",
    "朴素贝叶斯（Naive Bayes）是一种基于贝叶斯定理的分类算法，具有预测因子之间相互独立的假设。简单来说，朴素贝叶斯分类器假定特征之间是相互独立的，特征之间没有相互影响。\n",
    "朴素贝叶斯提供了一种使用 P(c)、P(x) 和 P(x|c) 来计算后验概率 P(c|x) 的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P(c|x)= [P(c)*P(x|c)]/P(x) \n",
    "P(c|x) 是后验概率\n",
    "P(c) 是先验概率\n",
    "P(x|c) 是给定类别下 c 下的是预测器 x 的概率\n",
    "P(x) 是预测器的概率\n",
    "朴素贝叶斯使用类似的方法来预测基于不同属性的不同类别的概率。该算法主要用于文本分类和多分类问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Library\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\n",
    "                                   # Create NB classification object\n",
    "model = GaussianNB()               # there is other distribution for multinomial classes like Bernoulli Naive Bayes, Refer link\n",
    "\n",
    "model.fit(X, y)                     # Train the model using the training sets and check score\n",
    "model.score(X, y)\n",
    "predicted= model.predict(x_test)    #Predict Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. k 近邻算法\n",
    "k 近邻算法（kNN）可以用于分类问题和回归问题，它在分类问题中得到了更广泛的应用。k 近邻是一种简单的算法，训练时存储所有样本数据，测试时使用距离衡量法，通过 k 个最近的邻居进行投票的方式对新样本进行分类。\n",
    "衡量距离的函数可以是 Euclidean 距离、Manhattan 距离、Minkowski 距离、Hamming 距离。前三个用于连续函数，第四个用于分类变量。如果 k＝1，则将该判定为与其最近邻的类。有时，选择合适的 k 值非常重要，通常可以使用交叉验证来选择。\n",
    "选择使用 k 近邻算法之前应该作以下考虑：\n",
    "1)k 近邻算法计算成本高\n",
    "2)自变量应该归一化，否则较大的数值范围会让模型产生偏差。\n",
    "3)去除样本集中的离群点和噪声"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Library\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\n",
    "                                         # Create KNeighbors classifier object model \n",
    "KNeighborsClassifier(n_neighbors=6)      # default value for n_neighbors is 5\n",
    "\n",
    "model.fit(X, y)                          # Train the model using the training sets and check score\n",
    "model.score(X, y)\n",
    "predicted= model.predict(x_test)         #Predict Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. k-Means\n",
    "k-Means 是一种解决聚类问题的无监督算法。其过程遵循一种简单易行的方法，通过特定数量的集群（例如 k 个集群）对给定的数据集进行分类。集群内的数据点是同一的，不同集群之间的数据点是非均一的。\n",
    "k-Means 算法流程：\n",
    "1.随机为每个集群挑选 k 个点，称为质心。\n",
    "2.每个数据选择与它最近的质心，并标记为该类。\n",
    "3.所有数据都聚类完之后，对每个类重新计算质心。\n",
    "4.当产生新的质心时，重复第 2 步和第 3 步。\n",
    "在 k-Means 中，我们有簇，每个簇都有它自己的质心。质心与该簇中的数据点之间的差的平方和构成了该簇的平方和。把所有簇的平方值和相加，就得到了该情况下总的平方和。\n",
    "我们知道，随着集群数量 k 的增加，总的平方和会持续减小，如果把总的平方和绘制出来，可能会看到先是急剧减少，直到选择某个值 k，下降缓慢得多。因此，我们可以根据这个找到最佳的簇数 k。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Library\n",
    "from sklearn.cluster import KMeans\n",
    "#Assumed you have, X (attributes) for training data set and x_test(attributes) of test_dataset\n",
    "\n",
    "k_means = KMeans(n_clusters=3, random_state=0)                  # Create KNeighbors classifier object model \n",
    "\n",
    "model.fit(X)                                                    # Train the model using the training sets and check score\n",
    "\n",
    "predicted= model.predict(x_test)                                # Predict Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 随机森林\n",
    "随机森林（Random Forest）是决策树集合。在随机森林中，我们收集了许多决策树（被称为“森林”）。为了根据属性对新对象进行分类，每个树都给出分类，然后对这些树的结果进行“投票”，最终选择投票得数最多的哪一类别。\n",
    "每棵树按以下方法构建：\n",
    "1.如果取 N 例训练样本作为来训练每棵树，则随机抽取 N 例样本，再有放回地进行下一次抽样,每次抽样得到的 N 个样本作为一棵树的训练数据。\n",
    "2.如果存在 M 个输入变量（特征值），则指定一个数字 m（远小于 M），使得在每个节点处，随机地从 M 中选择 m 个特征，并使用这些m 个特征来对节点进行最佳分割。在森林生长过程中，m 的值保持不变。\n",
    "3.每棵树都尽可能自由生长。没有修剪。\n",
    "关于此算法的更多细节，请参阅以下文章：\n",
    "https://www.analyticsvidhya.com/blog/2014/06/introduction-random-forest-simplified/\n",
    "(https://www.analyticsvidhya.com/blog/2014/06/comparing-cart-random-forest-1/)[https://www.analyticsvidhya.com/blog/2014/06/comparing-cart-random-forest-1/]\n",
    "https://www.analyticsvidhya.com/blog/2014/06/comparing-random-forest-simple-cart-model/\n",
    "https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Library\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\n",
    "\n",
    "model= RandomForestClassifier()                        # Create Random Forest object\n",
    "\n",
    "model.fit(X, y)                                        # Train the model using the training sets and check score\n",
    "model.score(X, y)\n",
    "predicted= model.predict(x_test)                       # Predict Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 降维算法(主成分分析PCA)\n",
    "在过去的 4-5 年，数据捕获在每一个可能的阶段都有指数增长。企业/政府机构/研究机构不仅提供新的数据来源，而且获取的数据也越来越详细。\n",
    "例如电子商务公司正在捕捉更多关于客户的细节，比如他们的人口统计、网络爬行历史、他们喜欢或不喜欢什么、购买历史、反馈和许多其他信息，以便比最近的杂货店老板更能给予他们个性化的关注。\n",
    "作为一名数据科学家，我们提供的数据包括许多特性，这听起来有利于建立良好的健壮模型，但是存在一个问题：你如何识别出 1000 或 2000 个特征中那些是最重要的呢？在这种情况下，降维算法可以帮助我们及其他各种算法，如决策树、随机森林、主成分分析、因子分析、基于相关矩阵的识别、缺失值比等。\n",
    "关于降维算法的更多内容，请参阅以下文章：\n",
    "https://www.analyticsvidhya.com/blog/2015/07/dimension-reduction-methods/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Library\n",
    "from sklearn import decomposition\n",
    "#Assumed you have training and test data set as train and test\n",
    "\n",
    "pca= decomposition.PCA(n_components=k)         # Create PCA obeject  \n",
    "                                               #default value of k =min(n_sample, n_features)\n",
    "                                               # For Factor analysis\n",
    "                                               #fa= decomposition.FactorAnalysis()\n",
    "\n",
    "train_reduced = pca.fit_transform(train)       # Reduced the dimension of training dataset using PCA\n",
    "\n",
    "test_reduced = pca.transform(test)             #Reduced the dimension of test dataset\n",
    "#For more detail on this, please refer  this link:http://scikit-learn.org/stable/modules/decomposition.html#decompositions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 梯度提升算法\n",
    "Boosting 实际上是一个学习算法的集合，它结合了几个基本估计量的预测，以便比单个估计量提高鲁棒性。它将多个弱或平均预测因子组合成一个强预测因子。这些升力算法在 Kaggle、AV Hakason、CrowdAnalytix 等数据科学竞赛中一直表现不错。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 GBM（gradient boosting machine）\n",
    "关于 GBM 的更多相关知识请参阅：\n",
    "https://www.analyticsvidhya.com/blog/2015/05/boosting-algorithms-simplified/\n",
    "\n",
    "梯度提升分类器和随机森林是两种不同的 boosting 树分类器，两种算法的区别请参阅：\n",
    "https://discuss.analyticsvidhya.com/t/what-is-the-fundamental-difference-between-randomforest-and-gradient-boosting-algorithms/2341"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Library\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\n",
    "                                                # Create Gradient Boosting Classifier object\n",
    "model= GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "                                                \n",
    "model.fit(X, y)                                 # Train the model using the training sets and check score\n",
    "model.score(X, y)\n",
    "\n",
    "predicted= model.predict(x_test)                #Predict Output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 XGBoost\n",
    "XGBoost 具有非常高的预测能力，这使它成为事件准确度的最佳选择，因为它同时具有线性模型和树学习算法，使得该算法比现有的梯度增强器技术快近10倍。\n",
    "XGBoost 支持包括各种目标函数，包括回归、分类和排序。\n",
    "XGBoost 最有趣的事情之一是它也被称为一种正则化的提升技术。这有助于减少过拟合。XGBoost 有许多语言提供支持，包括 Scala、Java、R、Python、Julia、C++ 等。\n",
    "XGBoost 支持分布式计算，包括 GCE，AWS，Azure 和 Yarn 集群。XGBoost 还可以与 Spark、Flink 和其他云数据流系统集成，并在每次提升过程的迭代中内置交叉验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "X = dataset[:,0:10]\n",
    "Y = dataset[:,10:]\n",
    "seed = 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=seed)\n",
    "\n",
    "model = XGBClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)                        #Make predictions for test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 LightGBM\n",
    "LightGBM 是一种基于树的学习算法的梯度提升框架。它被设计成分布式和高效的，具有以下优点：\n",
    "更快的训练速度和更高的效率\n",
    "占用内存少\n",
    "准确率高\n",
    "并行 GPU 支持\n",
    "能够处理大规模数据\n",
    "该框架是一种基于决策树算法的快速高效的梯度提升算法，用于排序、分类等机器学习任务。它是在微软的分布式机器学习工具包项目下开发的。\n",
    "由于 LightGBM 是基于决策树算法的，所以它以最佳拟合度按叶子方向分割树，而其他增强算法按层次或深度方向而不是按叶子方向分割树。因此，在 LightGBM 中，当在同一片叶子上生长时，叶子方向算法能够比逐级搜索（level-wise）算法减少更多的损耗，从而产生比现有任何提升算法都难以达到的更好的精度。\n",
    "此外，它的速度非常快，因此使用 Light（光）这个词。\n",
    "更多 LightGBM 知识，请参阅以下文章：\n",
    "https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.rand(500, 10)            # 500 entities, each contains 10 features\n",
    "label = np.random.randint(2, size=500)    # binary target\n",
    "\n",
    "train_data = lgb.Dataset(data, label=label)\n",
    "test_data = train_data.create_valid('test.svm')\n",
    "\n",
    "param = {'num_leaves':31, 'num_trees':100, 'objective':'binary'}\n",
    "param['metric'] = 'auc'\n",
    "\n",
    "num_round = 10\n",
    "bst = lgb.train(param, train_data, num_round, valid_sets=[test_data])\n",
    "\n",
    "bst.save_model('model.txt')\n",
    "\n",
    "                                          # 7 entities, each contains 10 features\n",
    "data = np.random.rand(7, 10)\n",
    "ypred = bst.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4 Catboost\n",
    "Catboost 是来自于 Yandex 的开源机器学习算法。它可以很容易地与谷歌的 TensorFlow 苹果的 Core ML 等深度学习框架相结合。\n",
    "CatBoost 最大的好处是它不需要像其他 ML 模型那样进行广泛的数据样本训练，而且可以处理各种数据格式，不会破坏模型的健壮性。\n",
    "在执行 CatBoost 之前，务必确保已经处理了缺失数据。\n",
    "Catboost 可以在不显示类型转换错误的情况下自动处理分类变量，这有助于集中精力更好地调优模型，而不是解决一些小错误。\n",
    "更多 Catboost 知识，请参阅以下文章：\n",
    "https://www.analyticsvidhya.com/blog/2017/08/catboost-automated-categorical-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "#Read training and testing files\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "#Imputing missing values for both train and test\n",
    "train.fillna(-999, inplace=True)\n",
    "test.fillna(-999,inplace=True)\n",
    "\n",
    "#Creating a training set for modeling and validation set to check model performance\n",
    "X = train.drop(['Item_Outlet_Sales'], axis=1)\n",
    "y = train.Item_Outlet_Sales\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=0.7, random_state=1234)\n",
    "categorical_features_indices = np.where(X.dtypes != np.float)[0]\n",
    "\n",
    "#importing library and building model\n",
    "from catboost import CatBoostRegressormodel=CatBoostRegressor(iterations=50, depth=3, learning_rate=0.1, loss_function='RMSE')\n",
    "\n",
    "model.fit(X_train, y_train,cat_features=categorical_features_indices,eval_set=(X_validation, y_validation),plot=True)\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['Item_Identifier'] = test['Item_Identifier']\n",
    "submission['Outlet_Identifier'] = test['Outlet_Identifier']\n",
    "submission['Item_Outlet_Sales'] = model.predict(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
